{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "(3) 앙상블(Ensemble)\n",
    "- 여러 개의 기본 모델(base model)을 결합하여 하나의 새로운 모델(ensemble model)을 생성하는 방법\n",
    "\n",
    "- 앙상블 알고리즘 특징\n",
    "> 기본 모델을 결합하여 전체 모델의 성능 향상을 목표로 함\n",
    "> 단일 모델을 사용할 때보다 편향(bias)과 분산(variance)을 모두 적절히 고려할 수 있음 -> 과적합 or 과소적합  방지에 용이함\n",
    "> 일반적으로 단일 모델의 성능보다 앙상블 모델의 성능이 우수함\n",
    "\n",
    "- 앙상블 학습 시 고려사항\n",
    "> “어떤 알고리즘을 사용할 것인가”  성능 개선에 효과적인 알고리즘을 선택하여 결합\n",
    "> “어떻게 결합할 것인가”  학습이 완료된 기본 모델로부터 얻어지는 결과를 각 모델의 특성을 고려하여 결합\n",
    "\n",
    "- 앙상블 학습 방법\n",
    "> 1) 병렬적 결합 : 각각의 기본 모델로부터 얻어진 결과를 한번에 고려하여 하나의 최종 결과를 얻음\n",
    "> 2) 순차적 결합 : 각 기본모델의 결과를 단계별로 나눠, 앞선 모델의 결과가 다른 모델의 학습에 영향을 미침\n",
    "\n",
    "1) 보팅(Voting)\n",
    "- 여러 모델을 통해 얻은 예측값들을 대상으로 투표를 하여 최종 클래스를 예측하는 앙상블 알고리즘\n",
    "\n",
    "- 알고리즘 특징\n",
    "> 서로 다른 알고리즘을 학습한 후, 예측값을 결합하는 방법\n",
    "> 하드 보팅과 소프트 보팅으로 분류\n",
    "> 하드 보팅에 비해 소프트 보팅이 각 모델의 예측값을 유연하게 적용 가능하기 때문에 보편적으로 사용\n",
    "\n",
    "(1) 하드 보팅 (Hard Voting): 각 모델의 예측 결과로 다수결 투표를 진행하여 출력한 결과가 앙상블 모델의 최종 출력값이 됨.\n",
    "(2) 소프트 보팅 (Soft Voting): 각 모델이 예측한 확률을 평균하여 클래스별 최종 확률 계산하고 최종적으로 가장 큰 확률을 가진 클래스를 출력함.\n",
    "\n",
    "2) 배깅(Bagging)\n",
    "- 부스트랩 샘플링을 통해 하나의 알고리즘을 학습하여 생성된 여러 모델의 결과를 결합하여 최종 예측하는 알고리즘\n",
    "\n",
    "- 알고리즘 특징\n",
    "> 부스트랩 결합(Boostrap aggregating)의 줄임말\n",
    "> 부스트랩 샘플링: 원 데이터에서 복원 추출을 통해 동일한 크기의 샘플을 다수 추출\n",
    "> 평균적으로 전체 데이터 중 63% 정도만 추출됨\n",
    "\n",
    "- 알고리즘 순서\n",
    "> 학습 데이터로부터 부스트랩 샘플링을 진행하여 부스트랩 데이터를 생성\n",
    "> 각 부스트랩 데이터로 다수의 개별 모델을 학습\n",
    "> 최종 예측을 위해 투표(Voting)를 진행 (일반적으로 타겟 변수(y)가 연속형일 때에는 평균, 범주형일 때에는 보팅을 사용)\n",
    "\n",
    "- 장점\n",
    "> 복원 샘플링을 통해 최종 모델의 분산을 줄여줌으로써 예측력 향상\n",
    "> 병렬 학습이 가능하여 모델 확장성이 좋음\n",
    "\n",
    "- 단점\n",
    "> 부스트래핑 시, 일부 샘플은 사용되지 않고 특정 샘플만 여러 번 사용되어 특정 데이터에 편향될 가능성 존재\n",
    "\n",
    "(3) 랜덤 포레스트\n",
    "- 다수의 의사결정나무(Decision Tree)를 학습 후 결합하여 최종 예측 결과를 도출하는 알고리즘\n",
    "\n",
    "- 알고리즘 특징\n",
    "> 부스트랩 샘플링으로 최소 노드 개수가 될 때까지 각 의사결정나무를 학습\n",
    "> 학습 데이터의 모든 독립변수를 사용하지 않고 각 의사결정나무에서 임의의 독립변수를 학습(feature sampling)\n",
    "> Out of bag(OOB): 학습에 사용되지 않은 데이터(약 전체의 1/3)는 모델 성능 검증에 사용\n",
    "\n",
    "- 알고리즘 순서\n",
    "> 학습 데이터로부터 부스트랩 샘플링을 통해 부스트랩 데이터를 생성\n",
    "> 각 부스트랩 데이터로 의사결정나무를 학습\n",
    "> 각 의사결정나무로부터 출력된 결과를 기반으로 간접 보팅/직접 보팅(분류) 또는 평균화(회귀)를 통해 최종 예측 결과를 도출\n",
    "\n",
    "(4) GBM\n",
    "- 이전 예측기가 만든 잔차로 새로운 예측기를 학습시키는 부스팅 알고리즘\n",
    "- 알고리즘 특징\n",
    "> Gradient Descent(경사하강법)와 Boosting을 합친 방법\n",
    "> 일반적인 부스팅 방법과 달리, 데이터에 대한 가중치가 없음\n",
    "> 잔차 (손실함수의 Negative Gradient, 𝑦−𝑦 ̂)를 학습\n",
    "> 일반적으로 GBM(Gradient Boosting Model)으로 불림\n",
    "> 기본 학습기로 의사결정나무를 사용하기 때문에, GBDT(Gradient Boosted Decision Trees)라고도 불림\n",
    "\n",
    "(5) XGBOOST\n",
    "- 병렬 학습이 지원되도록 구현한 GBM 계열 알고리즘으로, Extreme Gradient Boosting의 약자\n",
    "\n",
    "- 알고리즘 특징\n",
    "> Level-wise 분할 트리 구조 형성 : 트리의 깊이를 최소화하여 균형 잡힌 트리를 유지하며 분할  과적합 방지 but 느린 학습속도\n",
    "> 손실함수에 모델에 대한 규제항 추가\n",
    "> Leaf 노드에서 손실함수의 미분값으로 계산 가능한 Leaf score 값을 가짐\n",
    "> 최종 예측값(𝑦 ̂)은 Leaf score 값을 합산하여 계산\n",
    "\n",
    "- 장점\n",
    "> GPU 지원 및 병렬 처리로 인해 기존 GBM보다 학습속도가 빠른 편\n",
    "> 과적합 규제 가능\n",
    "> 높은 예측 성능을 가짐\n",
    "> Early Stopping(조기종료) 기능이 있음\n",
    "\n",
    "- 단점\n",
    "> 튜닝이 필요한 하이퍼파라미터의 개수가 많음\n",
    "> GBM 보다 학습속도가 빠르지만, 다른 머신러닝 모델들에 비해 학습속도가 느림\n",
    "\n",
    "(6) LightGBM\n",
    "- XGBoost의 학습속도를 개선한 GBM 계열 알고리즘으로, Light Gradient Boosting Model의 약자\n",
    "\n",
    "- 알고리즘 특징\n",
    "> GOSS(Gradient-based One-Side Sampling) : 학습 데이터의 관측치를 줄이는 기법\n",
    "> |Gradient|값이 큰(학습이 잘 안된) 관측치들은 남겨두고, |Gradient|값이 작은(학습이 잘된) 관측치들만 샘플링 진행\n",
    "> EFB(Exclusive Feature Bundling) : 학습 데이터의 독립변수들의 수를 줄이는 기법\n",
    "> 0 값을 많이 포함한(sparse) 변수들을 하나의 bundle로 묶어 변수 차원을 축소시킴\n",
    "> Leaf-Wise 분할 트리 구조 생성 : 트리의 균형을 맞추지 않고 leaf 노드를 지속적으로 분할  빠른 학습속도 but 과적합 가능\n",
    "\n",
    "- 장점\n",
    "> XGBoost에 비해 학습 속도가 빠름\n",
    "> 범주형 변수의 자동 변환 기능(Label 인코딩 지원)\n",
    "> 메모리 사용량이 적은 편\n",
    "> GPU 지원\n",
    "\n",
    "- 단점\n",
    "> 학습 데이터의 개수가 적으면 과적합 문제 발생 (10,000개 이상일 때 사용 권장)\n",
    "> 하이퍼파라미터 튜닝에 따른 성능 변화가 큼\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}