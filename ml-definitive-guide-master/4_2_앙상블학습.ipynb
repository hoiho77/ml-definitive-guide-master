{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "(3) 앙상블(Ensemble)\n",
    "- 여러 개의 기본 모델(base model)을 결합하여 하나의 새로운 모델(ensemble model)을 생성하는 방법\n",
    "\n",
    "- 앙상블 알고리즘 특징\n",
    "> 기본 모델을 결합하여 전체 모델의 성능 향상을 목표로 함\n",
    "> 단일 모델을 사용할 때보다 편향(bias)과 분산(variance)을 모두 적절히 고려할 수 있음 -> 과적합 or 과소적합  방지에 용이함\n",
    "> 일반적으로 단일 모델의 성능보다 앙상블 모델의 성능이 우수함\n",
    "\n",
    "- 앙상블 학습 시 고려사항\n",
    "> “어떤 알고리즘을 사용할 것인가”  성능 개선에 효과적인 알고리즘을 선택하여 결합\n",
    "> “어떻게 결합할 것인가”  학습이 완료된 기본 모델로부터 얻어지는 결과를 각 모델의 특성을 고려하여 결합\n",
    "\n",
    "- 앙상블 학습 방법\n",
    "> 1) 병렬적 결합 : 각각의 기본 모델로부터 얻어진 결과를 한번에 고려하여 하나의 최종 결과를 얻음\n",
    "> 2) 순차적 결합 : 각 기본모델의 결과를 단계별로 나눠, 앞선 모델의 결과가 다른 모델의 학습에 영향을 미침\n",
    "\n",
    "1) 보팅(Voting)\n",
    "- 여러 모델을 통해 얻은 예측값들을 대상으로 투표를 하여 최종 클래스를 예측하는 앙상블 알고리즘\n",
    "\n",
    "- 알고리즘 특징\n",
    "> 서로 다른 알고리즘을 학습한 후, 예측값을 결합하는 방법\n",
    "> 하드 보팅과 소프트 보팅으로 분류\n",
    "> 하드 보팅에 비해 소프트 보팅이 각 모델의 예측값을 유연하게 적용 가능하기 때문에 보편적으로 사용\n",
    "\n",
    "(1) 하드 보팅 (Hard Voting): 각 모델의 예측 결과로 다수결 투표를 진행하여 출력한 결과가 앙상블 모델의 최종 출력값이 됨.\n",
    "(2) 소프트 보팅 (Soft Voting): 각 모델이 예측한 확률을 평균하여 클래스별 최종 확률 계산하고 최종적으로 가장 큰 확률을 가진 클래스를 출력함.\n",
    "\n",
    "2) 배깅(Bagging)\n",
    "- 부스트랩 샘플링을 통해 하나의 알고리즘을 학습하여 생성된 여러 모델의 결과를 결합하여 최종 예측하는 알고리즘\n",
    "\n",
    "- 알고리즘 특징\n",
    "> 부스트랩 결합(Boostrap aggregating)의 줄임말\n",
    "> 부스트랩 샘플링: 원 데이터에서 복원 추출을 통해 동일한 크기의 샘플을 다수 추출\n",
    "> 평균적으로 전체 데이터 중 63% 정도만 추출됨\n",
    "\n",
    "- 알고리즘 순서\n",
    "> 학습 데이터로부터 부스트랩 샘플링을 진행하여 부스트랩 데이터를 생성\n",
    "> 각 부스트랩 데이터로 다수의 개별 모델을 학습\n",
    "> 최종 예측을 위해 투표(Voting)를 진행 (일반적으로 타겟 변수(y)가 연속형일 때에는 평균, 범주형일 때에는 보팅을 사용)\n",
    "\n",
    "- 장점\n",
    "> 복원 샘플링을 통해 최종 모델의 분산을 줄여줌으로써 예측력 향상\n",
    "> 병렬 학습이 가능하여 모델 확장성이 좋음\n",
    "\n",
    "- 단점\n",
    "> 부스트래핑 시, 일부 샘플은 사용되지 않고 특정 샘플만 여러 번 사용되어 특정 데이터에 편향될 가능성 존재\n",
    "\n",
    "(3) 랜덤 포레스트\n",
    "- 다수의 의사결정나무(Decision Tree)를 학습 후 결합하여 최종 예측 결과를 도출하는 알고리즘\n",
    "\n",
    "- 알고리즘 특징\n",
    "> 부스트랩 샘플링으로 최소 노드 개수가 될 때까지 각 의사결정나무를 학습\n",
    "> 학습 데이터의 모든 독립변수를 사용하지 않고 각 의사결정나무에서 임의의 독립변수를 학습(feature sampling)\n",
    "> Out of bag(OOB): 학습에 사용되지 않은 데이터(약 전체의 1/3)는 모델 성능 검증에 사용\n",
    "\n",
    "- 알고리즘 순서\n",
    "> 학습 데이터로부터 부스트랩 샘플링을 통해 부스트랩 데이터를 생성\n",
    "> 각 부스트랩 데이터로 의사결정나무를 학습\n",
    "> 각 의사결정나무로부터 출력된 결과를 기반으로 간접 보팅/직접 보팅(분류) 또는 평균화(회귀)를 통해"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}